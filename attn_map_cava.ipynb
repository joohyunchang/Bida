{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59302584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import deepspeed\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import json\n",
    "import os\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "from dataset.datasets import build_dataset\n",
    "import models.ast_clip_cast\n",
    "from run_bidirection_compo import get_args\n",
    "from timm.models import create_model\n",
    "import util_tools.utils as utils\n",
    "\n",
    "def main(args, ds_init):\n",
    "    device = torch.device(args.device)\n",
    "\n",
    "    args.audio_path = None if 'all' in args.ucf101_type else args.audio_path\n",
    "    \n",
    "    seed = args.seed + utils.get_rank()\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    dataset_val, args.nb_classes = build_dataset(is_train=False, test_mode=False, args=args)\n",
    "\n",
    "    patch_size = 14\n",
    "    print(\"Patch size = %s\" % str(patch_size))\n",
    "    args.window_size = 16\n",
    "    args.patch_size = patch_size\n",
    "    model_args = {\n",
    "            'model_name': args.vmae_model,\n",
    "            'pretrained': False,\n",
    "            'num_classes': args.nb_classes,\n",
    "            'all_frames': args.num_frames * args.num_segments,\n",
    "            'tubelet_size': args.tubelet_size,\n",
    "            'drop_rate': args.drop,\n",
    "            'drop_path_rate': args.drop_path,\n",
    "            'attn_drop_rate': args.attn_drop_rate,\n",
    "            'drop_block_rate': None,\n",
    "            'use_mean_pooling': args.use_mean_pooling,\n",
    "            'init_scale': args.init_scale,\n",
    "            'fusion_method': args.fusion_method\n",
    "        }\n",
    "    if args.audio_path is not None:\n",
    "        print(f\"Audio_Patch size = {args.audio_height*args.audio_width//(args.window_size*args.window_size)}\")\n",
    "        model_args['audio_patch'] = args.audio_height*args.audio_width//(args.window_size*args.window_size)\n",
    "    if args.bcast_method is not None:\n",
    "        print(f\"bcast_method = {args.bcast_method}\")\n",
    "        model_args['bcast_method'] = args.bcast_method\n",
    "    if args.time_encoding:\n",
    "        model_args['time_encoding'] = args.time_encoding\n",
    "        model_args['spec_shape'] = [args.audio_height//args.window_size, args.audio_width//args.window_size]\n",
    "    if args.audio_only_finetune:\n",
    "        model_args['audio_only_finetune'] = True\n",
    "    if '_ast_' in args.vmae_model:\n",
    "        model_args['fstride'] = args.stride\n",
    "        model_args['tstride'] = args.stride\n",
    "        model_args['input_fdim'] = args.audio_height\n",
    "        model_args['input_tdim'] = args.audio_width\n",
    "    if args.enable_audio_stride:\n",
    "        model_args['fstride'] = 10\n",
    "        fdim, tdim = int((args.audio_height-16)/10)+1, int((args.audio_width-16)/10)+1\n",
    "        model_args['audio_patch'] = fdim * tdim\n",
    "        model_args['spec_shape'] = [fdim, tdim]\n",
    "    if args.not_use_stpos == False:\n",
    "        model_args['use_stpos'] = args.not_use_stpos\n",
    "    if args.pre_time_encoding == True:\n",
    "        model_args['pre_time_encoding'] = args.pre_time_encoding\n",
    "    if args.split_time_mlp == True:\n",
    "        model_args['split_time_mlp'] = args.split_time_mlp\n",
    "    model_args['use_Adapter'] = True\n",
    "    model = create_model(**model_args)\n",
    "    model.to(device)\n",
    "\n",
    "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Model = %s\" % str(model))\n",
    "    print('number of params:', n_parameters)\n",
    "\n",
    "    checkpoint = torch.load(args.fine_tune, map_location=device)\n",
    "    model.load_state_dict(checkpoint['module'], strict=True)\n",
    "    \n",
    "    return args, model, dataset_val\n",
    "\n",
    "\n",
    "data_sel = 2\n",
    "if data_sel == 1:\n",
    "    ANNO_PATH='/data/joohyun7u/project/CAST/dataset/epic_sounds'\n",
    "    DATA_PATH='/local_datasets/epic_sounds/video'\n",
    "    AUDIO_PATH='/local_datasets/epic_sounds/wav'\n",
    "    DATA_SET='EPIC_sounds'\n",
    "    CLASS='44'\n",
    "    WIDTH='400'\n",
    "    VMAE_PATH='/data/datasets/Epickitchens100_clips/epic_checkpoint-2400.pth'\n",
    "else:\n",
    "    ANNO_PATH='/data/joohyun7u/project/CAST/dataset/vggsound'\n",
    "    DATA_PATH='/local_datasets/vggsound/resize256/'\n",
    "    AUDIO_PATH='/local_datasets/vggsound/resize256/'\n",
    "    DATA_SET='VGGSound'\n",
    "    CLASS='309'\n",
    "    WIDTH='1024'\n",
    "    VMAE_PATH='/data/dataset/epic_audio/vit_b_hybrid_pt_800e.pth'\n",
    "\n",
    "CLIP_PATH='/data/datasets/Epickitchens100_clips/ViT-B-16.pt'\n",
    "OUTPUT_DIR='/data/joohyun7u/project/CAST/xlog'\n",
    "    \n",
    "input_args = [\n",
    "'--data_set', DATA_SET,\n",
    "'--nb_classes', CLASS,\n",
    "'--data_path', DATA_PATH,\n",
    "'--anno_path', ANNO_PATH,\n",
    "'--vmae_finetune', VMAE_PATH,\n",
    "'--clip_finetune', CLIP_PATH,\n",
    "'--log_dir', OUTPUT_DIR,\n",
    "'--output_dir', OUTPUT_DIR,\n",
    "'--batch_size', '6',\n",
    "'--input_size', '224',\n",
    "'--short_side_size', '224',\n",
    "'--save_ckpt_freq', '10',\n",
    "'--num_sample', '1',\n",
    "'--num_frames', '16',\n",
    "'--opt', 'adamw',\n",
    "'--lr', '5e-4',\n",
    "'--opt_betas', '0.9', '0.999',\n",
    "'--layer_decay', '0.8',\n",
    "'--weight_decay', '0.05',\n",
    "'--epochs', '50',\n",
    "'--dist_eval',\n",
    "'--test_num_segment', '2',\n",
    "'--test_num_crop', '3',\n",
    "'--num_workers', '16',\n",
    "'--seed', '0',\n",
    "'--warmup_epochs', '5',\n",
    "'--enable_deepspeed',\n",
    "'--reprob', '0.',\n",
    "'--init_scale', '1.',\n",
    "'--unfreeze_layers', 'cross', 'clip_temporal_embedding', 'space_time_pos', 'vmae_fc_norm', 'last_proj', 'Adapter', 'ln_post', 'head', 'concat_head', 'clip_text_positional_embedding', 'time_mlp',\n",
    "'--update_freq', '2',\n",
    "'--drop_path', '0.2',\n",
    "'--head_drop', '0.',\n",
    "'--cutmix', '0.',\n",
    "'--mixup_switch_prob', '0',\n",
    "'--mixup_prob', '0.9',\n",
    "'--device','cuda',\n",
    "]\n",
    "audio_args = [\n",
    "'--vmae_model', 'single_ast_clip_vit_base_patch16_224',\n",
    "'--audio_path', AUDIO_PATH,\n",
    "'--audio_type', 'single',\n",
    "'--realtime_audio',\n",
    "'--audio_height', '128',\n",
    "'--audio_width', WIDTH,\n",
    "# '--spec_augment',\n",
    "'--specnorm',\n",
    "'--process_type', 'ast',\n",
    "'--split_time_mlp',\n",
    "# '--mask_audio_token', '0.25',\n",
    "# \"--ablation_eval\", \"missing\"\n",
    "'--time_encoding',\n",
    "'--not_use_stpos',\n",
    "]\n",
    "datasel2_audio_args = [\n",
    "'--vmae_model', 'single_ast_clip_vit_base_patch16_224',\n",
    "'--audio_path', AUDIO_PATH,\n",
    "'--audio_type', 'single',\n",
    "'--realtime_audio',\n",
    "'--audio_height', '128',\n",
    "'--audio_width', WIDTH,\n",
    "'--spec_augment',\n",
    "'--specnorm',\n",
    "'--process_type', 'ast',\n",
    "'--mixup_spec',\n",
    "'--add_noise',\n",
    "'--spec_cutmix',\n",
    "# '--mask_audio_token', '0.25',\n",
    "# \"--ablation_eval\", \"missing\"\n",
    "]\n",
    "\n",
    "if data_sel == 1:\n",
    "    # 8layer 쯤에 피크가 있음\n",
    "    time_args = input_args + audio_args + ['--fine_tune','/data/joohyun7u/project/CAST/log4/epic_sounds/EKSound_ASTCLIP_split_time_encoding_v5_MixSpecCut_AddNoise_UF4/OUT/checkpoint-best/mp_rank_00_model_states.pt']\n",
    "    # time_args = input_args + audio_args + ['--fine_tune','/data/joohyun7u/project/CAST/log4/epic_sounds/EKSound_ASTCLIP_time_encoding_v5_MixSpecCut_AddNoise_UF6/OUT/checkpoint-best/mp_rank_00_model_states.pt']\n",
    "else:\n",
    "    time_args = input_args + datasel2_audio_args + ['--time_encoding', '--split_time_mlp', '--not_use_stpos','--fine_tune','/data/joohyun7u/project/CAST/log4/vggsound/VGGSound_ASTCLIP_split_time_encoding_v5_VMHY_MixSpecCut_AddNoise/OUT/checkpoint-best/mp_rank_00_model_states.pt']\n",
    "    # time_args = input_args + datasel2_audio_args + ['--fine_tune','/data/joohyun7u/project/CAST/log3/vggsound/VGGSound_ASTCLIP_VMHY_MixSpecCut_AddNoise/OUT/checkpoint-best/mp_rank_00_model_states.pt']\n",
    "    # time_args = input_args + datasel2_audio_args + ['--time_encoding','--fine_tune','/data/joohyun7u/project/CAST/log3/vggsound/VGGSound_ASTCLIP_VMHY_MixSpec_AddNoise/OUT/checkpoint-best/mp_rank_00_model_states.pt']\n",
    "# sys.argv를 수정하여 인자 전달\n",
    "import sys\n",
    "original_argv = sys.argv.copy()\n",
    "sys.argv = ['attn_map.py'] + time_args\n",
    "\n",
    "opts, ds_init = get_args()\n",
    "time_args, time_model, data_loader_val = main(opts, ds_init)\n",
    "\n",
    "# 원래 argv 복원\n",
    "sys.argv = original_argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c3189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from util_tools import audio_transforms\n",
    "import importlib\n",
    "importlib.reload(audio_transforms)\n",
    "from util_tools.audio_transforms import Spectrogram\n",
    "import os, torchaudio\n",
    "import IPython.display as ipd\n",
    "# from ipywidgets import Video\n",
    "from einops import rearrange\n",
    "    \n",
    "def tensor_to_img(spectrogram,output=None):\n",
    "    # plt.figure(figsize=(spectrogram.shape[-1] // 100, spectrogram.shape[-2] // 100))\n",
    "    spectrogram = rearrange(spectrogram,'h w -> w h')\n",
    "    cax = plt.pcolormesh(spectrogram, shading='auto')  # Use 'shading' for better color interpolation\n",
    "    plt.ylabel('Frequency [Hz]')\n",
    "    plt.xlabel('Time [sec]')\n",
    "    \n",
    "    plt.colorbar(cax)  # This adds the color bar to the right of the plot\n",
    "    plt.show()\n",
    "    display(spectrogram.shape)\n",
    "    if output:\n",
    "        return spectrogram\n",
    "    \n",
    "def show_wav(sample, sr):\n",
    "    plt.plot(sample.t().numpy())\n",
    "    plt.title('Waveform')\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "    \n",
    "    samples = sample[0]\n",
    "    fft_result = np.fft.fft(samples.numpy())\n",
    "    fft_freq = np.fft.fftfreq(len(samples), 1 / sr)\n",
    "\n",
    "    # FFT 결과 시각화\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(fft_freq, np.abs(fft_result))\n",
    "    plt.title('Frequency Domain')\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.xlim(0, sr / 2)  # Nyquist Frequency까지 표시\n",
    "    plt.show()\n",
    "    \n",
    "def show_wav_compare(original_sample, noise_reduced_sample=None, sr=None):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(original_sample.t().numpy())\n",
    "    if noise_reduced_sample is not None:\n",
    "        plt.plot(noise_reduced_sample, alpha=1)\n",
    "    plt.title('Original & Noise Reduced Audio Waveform')\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Amplitude')    \n",
    "    \n",
    "def display_audio(sample_id=None, audio_path = '/data/datasets/epic_audio/wav_split/', video_path='/data/datasets/Epickitchens100_clips/videos', data_set='EPIC' ,log=False, log_spec=False, preds=None, log_wav=False,output=None):\n",
    "    if data_set=='EPIC_sounds':\n",
    "        audio_path = os.path.join('/data/datasets/epic_sounds/wav/', sample_id + '.wav')\n",
    "        video_path = os.path.join('/data/datasets/epic_sounds/video/', sample_id + '.mp4')\n",
    "    else:    \n",
    "        audio_path = os.path.join(audio_path)\n",
    "        # video_path = os.path.join(video_path)\n",
    "    if log:\n",
    "        print(audio_path,'\\n',video_path)\n",
    "    \n",
    "    samples, sample_rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    samples_numpy = show_wav_compare(samples, sr=sample_rate)\n",
    "    display(ipd.Audio(samples, rate=sample_rate))\n",
    "    if output:\n",
    "        return samples\n",
    "    \n",
    "    # video = Video.from_file(video_path)\n",
    "    # display(ipd.display(video))\n",
    "    \n",
    "f_dim, t_dim = time_model.get_shape(time_args.stride, time_args.stride, time_args.audio_height, time_args.audio_width)\n",
    "T = time_args.num_frames//2\n",
    "f_dim, t_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f880f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader_val.spectrogram.noisereduce = True\n",
    "data_loader_val.spectrogram.noisereduce = False\n",
    "data_loader_val.sampling_type = 'uniform'\n",
    "# data_loader_val.sampling_type = 'dense'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a0f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_model.eval()\n",
    "device='cuda'\n",
    "\n",
    "from models.prompt import dataset_class\n",
    "action = dataset_class(time_args.data_set, time_args.anno_path)\n",
    "\n",
    "# data_iter = iter(data_loader_val)\n",
    "# batch = next(data_iter)\n",
    "# batch = data_loader_val.get_item_by_index(42)\n",
    "# batch = data_loader_val.get_item_by_index(922)\n",
    "# batch = data_loader_val.get_item_by_index(6922)\n",
    "# batch = data_loader_val.get_item_by_index(4822)\n",
    "# batch = data_loader_val.get_item_by_index(8000)  # PPT로 만들러짐\n",
    "# batch = data_loader_val.get_item_by_index(2222)\n",
    "# batch = data_loader_val2.get_item_by_index(1252)\n",
    "# ek-sound \n",
    "# batch = data_loader_val.get_item_by_index(2340)\n",
    "# batch = data_loader_val.get_item_by_index(1507)\n",
    "# batch = data_loader_val.get_item_by_index(1326)\n",
    "# batch = data_loader_val.get_item_by_index(5722) \n",
    "# batch = data_loader_val.get_item_by_index(626) # 별로임\n",
    "\n",
    "# #vggsound\n",
    "\n",
    "# batch = data_loader_val.get_item_by_index(9542)\n",
    "# batch = data_loader_val.get_item_by_index(7422) # Cat 안대 후반부에 나쁘지 않을지도\n",
    "# batch = data_loader_val.get_item_by_index(9569) # Dog\n",
    "batch = data_loader_val.get_item_by_index(6929)\n",
    "# batch = data_loader_val.get_item_by_index(8643) # 이건 애매함\n",
    "\n",
    "import random\n",
    "\n",
    "random_idx = random.randint(0, len(data_loader_val) - 1)  # 정수 인덱스\n",
    "# batch = data_loader_val.get_item_by_index(random_idx)\n",
    "# batch = data_loader_val.get_item_by_index(9569)\n",
    "\n",
    "\n",
    "time_idx = torch.tensor(batch[5]).unsqueeze(0)\n",
    "print(batch[2],time_idx)\n",
    "samples = batch[0]\n",
    "target = batch[1]\n",
    "batch_size = samples.shape[0]\n",
    "samples = samples.to(device, non_blocking=True).unsqueeze(0)\n",
    "if time_args.audio_path is not None:\n",
    "    if time_args.collate:\n",
    "        spec = [spe.to(device, non_blocking=True).half() for spe in batch[3]]\n",
    "    else:\n",
    "        spec = batch[3].to(device, non_blocking=True).unsqueeze(0)\n",
    "else:\n",
    "    spec = None\n",
    "captions = batch[4]\n",
    "ste = time_args.audio_width // 16\n",
    "\n",
    "# time_idx = time_idx.reshape(time_idx.shape[0],-1,2).to(dtype=samples.dtype, device=samples.device)\n",
    "# start_end = time_idx[:,0,:]\n",
    "# linspace = torch.linspace(0, 1, steps=ste+1).to(dtype=samples.dtype, device=samples.device)\n",
    "# segments = start_end[:, 0:1] + (start_end[:, 1:2] - start_end[:, 0:1]) * linspace[:-1]\n",
    "# next_segments = start_end[:, 0:1] + (start_end[:, 1:2] - start_end[:, 0:1]) * linspace[1:]\n",
    "# segments = torch.stack([segments, next_segments], dim=-1).view(time_idx.shape[0], ste, 2)\n",
    "\n",
    "if DATA_SET in [\"EPIC_sounds\"]:\n",
    "    path = os.path.join(time_args.audio_path, batch[2] + '.wav')\n",
    "    video_path = os.path.join(time_args.data_path, batch[2] + '.mp4')\n",
    "else:\n",
    "    path = os.path.join(time_args.audio_path, batch[2] + '.mp4')\n",
    "    video_path = os.path.join(time_args.data_path, batch[2] + '.mp4')\n",
    "\n",
    "audio_spec = tensor_to_img(spec[0,0,:,:].cpu(),output=True)\n",
    "\n",
    "\n",
    "f_dim, t_dim = time_model.get_shape(time_args.stride, time_args.stride, time_args.audio_height, time_args.audio_width)\n",
    "\n",
    "logits, all_atts = time_model(samples[:1], caption=captions, spec=spec[:1], idx=time_idx[:1], output_attentions=True)\n",
    "\n",
    "print('random_idx',random_idx)\n",
    "print('action :', action['action'][batch[1]], '\\tpred :', action['action'][torch.argmax(logits).item()])\n",
    "audio_wav = display_audio(audio_path=path,output=True)\n",
    "video_numpy = batch[0].permute(1, 2, 3, 0)[::2,:,:,:].numpy()  # shape [16, 224, 224, 3]\n",
    "\n",
    "# Concatenate the frames horizontally\n",
    "concatenated_frames = np.concatenate(video_numpy, axis=1)\n",
    "\n",
    "# Plot the concatenated frames\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.imshow((concatenated_frames - concatenated_frames.min()) / (concatenated_frames.max() - concatenated_frames.min()))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "    \n",
    "depth = len(all_atts) // 2\n",
    "\n",
    "\n",
    "t2s_all_attn = []\n",
    "s2t_all_attn = []\n",
    "for layer_id in range(depth):\n",
    "\n",
    "    # T2S: audio Q, video K (e.g. [1, 12, 468, 1568])\n",
    "    t2s_all_attn.append(all_atts[2 * layer_id])\n",
    "\n",
    "    # S2T: video Q, audio K (e.g. [1, 12, 1568, 468])\n",
    "    s2t_all_attn.append(all_atts[2 * layer_id + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f647ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def make_save_dir(batch, action, base_dir=\"results\"):\n",
    "    \"\"\"\n",
    "    build folder name: <video_uid>_<action(with-hyphen)>_<frame_idx>\n",
    "    e.g.  1234_open-door_17\n",
    "    \"\"\"\n",
    "    folder = f\"{batch[2]}_{action['action'][batch[1]].replace(' ', '-')}_{batch[1]}_sampling\"\n",
    "    save_dir = os.path.join(base_dir, folder)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    return save_dir\n",
    "\n",
    "def save_audio_spec(audio_spec, save_path):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(audio_spec, aspect=\"auto\", origin=\"lower\", cmap=\"gray_r\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches=\"tight\", pad_inches=0)\n",
    "    plt.close()\n",
    "    \n",
    "def _prep_rgb(img):\n",
    "    \"\"\"RGB [H,W,3] → numpy.float32 in [0,1]\"\"\"\n",
    "    if torch.is_tensor(img):\n",
    "        img = img.detach().cpu().float().numpy()\n",
    "    if img.dtype == np.uint8:          # 이미 0–255 uint8\n",
    "        return img\n",
    "    # float32/64 → 0–1 로 스케일 or 클리핑\n",
    "    if img.min() < 0 or img.max() > 1:\n",
    "        img = (img - img.min()) / (img.max() - img.min() + 1e-6)\n",
    "    return np.clip(img, 0.0, 1.0)\n",
    "def save_rgb_image(img, save_path):\n",
    "    img = _prep_rgb(img)\n",
    "    plt.figure(figsize=(32, 4))\n",
    "    plt.imshow(img, aspect=\"auto\"); plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches=\"tight\", pad_inches=0)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06096143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch, torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_s2t_attention_overlay(audio_spec: torch.Tensor,\n",
    "                               attn_audio: torch.Tensor,\n",
    "                               *,\n",
    "                               mode: str = \"per_frame\",   # \"per_frame\" | \"mean\"\n",
    "                               frames: list[int] | None = None,  # 시각화할 vt 인덱스\n",
    "                               alpha: float = 0.4,\n",
    "                               cmap: str = \"jet\",\n",
    "                               save_path=None,):\n",
    "    \"\"\"\n",
    "    audio_spec : [128, 400]  — Mel-spectrogram\n",
    "    attn_audio : [8, 12, 39] — mean-pooled S2T attention (vt, f_dim, t_dim)\n",
    "\n",
    "    mode = \"per_frame\" → 프레임마다 overlay (기존 방식)\n",
    "    mode = \"mean\"      → vt 차원을 평균 내어 하나의 히트맵 overlay\n",
    "    frames             → \"per_frame\"에서 보고 싶은 vt 인덱스 지정\n",
    "                         (None 이면 전 프레임)\n",
    "    \"\"\"\n",
    "    if mode not in {\"per_frame\", \"mean\"}:\n",
    "        raise ValueError(\"mode must be 'per_frame' or 'mean'\")\n",
    "\n",
    "    # --- attention upsampling ---\n",
    "    attn_up = F.interpolate(attn_audio.unsqueeze(1), size=audio_spec.shape,\n",
    "                            mode=\"bilinear\", align_corners=False).squeeze(1)    # [8,128,400]\n",
    "\n",
    "    # --- 정규화 (0-1) : vt 별 또는 평균 맵 하나 ---\n",
    "    if mode == \"per_frame\":\n",
    "        min_val = torch.amin(attn_up, dim=(-2, -1), keepdim=True)\n",
    "        max_val = torch.amax(attn_up, dim=(-2, -1), keepdim=True)\n",
    "        attn_up = (attn_up - min_val) / (max_val - min_val + 1e-6)\n",
    "        vt_list = frames if frames is not None else list(range(attn_up.size(0)))\n",
    "        n = len(vt_list)\n",
    "        cols = min(4, n)                    # 줄당 최대 4장\n",
    "        rows = math.ceil(n / cols)\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 3*rows),\n",
    "                                 sharex=True, sharey=True)\n",
    "        axes = axes.flatten() if n > 1 else [axes]\n",
    "        for ax_i, vt in enumerate(vt_list):\n",
    "            ax = axes[ax_i]\n",
    "            ax.imshow(audio_spec.cpu(), aspect=\"auto\", origin=\"lower\", cmap=\"gray_r\")\n",
    "            ax.imshow(attn_up[vt].cpu(), aspect=\"auto\", origin=\"lower\",\n",
    "                      cmap=cmap, alpha=alpha)\n",
    "            # ax.set_title(f\"vt = {vt}\", fontsize=10)\n",
    "            ax.axis(\"off\")\n",
    "        # 빈 서브플럿 숨김\n",
    "        for ax in axes[n:]:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    else:  # mode == \"mean\"\n",
    "        attn_mean = attn_up.mean(0, keepdim=True)          # [1,128,400]\n",
    "        attn_mean = (attn_mean - attn_mean.min()) / (attn_mean.max() - attn_mean.min() + 1e-6)\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        ax.imshow(audio_spec.cpu(), aspect=\"auto\", origin=\"lower\", cmap=\"gray_r\")\n",
    "        ax.imshow(attn_mean[0].cpu(), aspect=\"auto\", origin=\"lower\",\n",
    "                  cmap=cmap, alpha=alpha)\n",
    "        # ax.set_title(\"Mean of all frames\", fontsize=12)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches=\"tight\", pad_inches=0)\n",
    "    plt.show()\n",
    "    if save_path is not None:\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "\n",
    "for layer_id in range(depth):\n",
    "    print(f\"Layer {layer_id+1}\")\n",
    "    s2t = s2t_all_attn[layer_id]\n",
    "    s2t_split = rearrange(s2t,'b h (vn vt) (af at) -> b h vt vn af at',vt=T, vn=196, af=f_dim, at=t_dim)\n",
    "    s2t_split_attn_audio = s2t_split.mean(-3).mean(1)\n",
    "    \n",
    "    save_dir = make_save_dir(batch, action, base_dir=\"./results\")\n",
    "    base_name = f\"layer{layer_id+1:02d}\"\n",
    "\n",
    "    # 1) 원본 스펙트로그램\n",
    "    # spec_path = os.path.join(save_dir, f\"spec.png\")\n",
    "    # save_audio_spec(audio_spec.detach(), spec_path)\n",
    "\n",
    "    # 2) overlay (mean 모드 예시)\n",
    "    # overlay_path = os.path.join(save_dir, f\"{base_name}_spec_overlay.png\")\n",
    "    show_s2t_attention_overlay(audio_spec.detach(),\n",
    "                               s2t_split_attn_audio[0].detach(),\n",
    "                               mode='mean',\n",
    "                            #    save_path=overlay_path\n",
    "                               )\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "video_path = Path(time_args.data_path) / (batch[2] + '.mp4')\n",
    "shutil.copy2(video_path, save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a5061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch, torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange               # pip install einops\n",
    "\n",
    "def _prep_rgb(img):\n",
    "    \"\"\"RGB [H,W,3] → numpy.float32 in [0,1]\"\"\"\n",
    "    if torch.is_tensor(img):\n",
    "        img = img.detach().cpu().float().numpy()\n",
    "    if img.dtype == np.uint8:          # 이미 0–255 uint8\n",
    "        return img\n",
    "    # float32/64 → 0–1 로 스케일 or 클리핑\n",
    "    if img.min() < 0 or img.max() > 1:\n",
    "        img = (img - img.min()) / (img.max() - img.min() + 1e-6)\n",
    "    return np.clip(img, 0.0, 1.0)\n",
    "\n",
    "def show_t2s_attention_overlay(concat_frames: torch.Tensor | np.ndarray,\n",
    "                               attn_video: torch.Tensor,\n",
    "                               *,\n",
    "                               mode: str = \"per_ts\",      # \"per_ts\" | \"mean\"\n",
    "                               ts: list[int] | None = None,   # 보고 싶은 at 인덱스\n",
    "                               alpha: float = 0.4,\n",
    "                               cmap: str = \"jet\",\n",
    "                               save_path=None):\n",
    "    \"\"\"\n",
    "    concat_frames : [224, 1792, 3]  (8 장의 224×224 프레임을 가로로 이어 붙인 RGB 이미지)\n",
    "    attn_video    : [39, 8, 196]    (at = 39, vt = 8, vn = 14×14)\n",
    "    \"\"\"\n",
    "    if mode not in {\"per_ts\", \"mean\"}:\n",
    "        raise ValueError(\"mode must be 'per_ts' or 'mean'\")\n",
    "\n",
    "    H, W = concat_frames.shape[:2]\n",
    "    device  = attn_video.device\n",
    "    concat_frames = _prep_rgb(concat_frames) \n",
    "    # (at, vt, vn=196) → (at, 14, 14*8=112)  패치 격자를 가로로 이어붙인 형태\n",
    "    attn_grid = rearrange(attn_video, 'at vt (ph pw) -> at ph (vt pw)', ph=14, pw=14)\n",
    "    # attn_grid = rearrange(attn_video, 'at vt (pw ph) -> at ph (vt pw)', ph=14, pw=14)\n",
    "    # ↑ [39,14,112]\n",
    "\n",
    "    # 업샘플 → [at, H, W]\n",
    "    attn_up = F.interpolate(attn_grid.unsqueeze(1), size=(H, W),\n",
    "                            mode=\"bilinear\", align_corners=False).squeeze(1)\n",
    "\n",
    "    if mode == \"per_ts\":\n",
    "        ts_list = ts if ts is not None else list(range(attn_up.size(0)))\n",
    "        # at 별로 0–1 정규화\n",
    "        min_val = torch.amin(attn_up[ts_list], dim=(-2, -1), keepdim=True)\n",
    "        max_val = torch.amax(attn_up[ts_list], dim=(-2, -1), keepdim=True)\n",
    "        attn_up = (attn_up[ts_list] - min_val) / (max_val - min_val + 1e-6)\n",
    "\n",
    "        n = len(ts_list)\n",
    "        cols = min(4, n); rows = math.ceil(n / cols)\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 3*rows),\n",
    "                                 sharex=True, sharey=True)\n",
    "        axes = axes.flatten() if n > 1 else [axes]\n",
    "\n",
    "        for ax_i, t in enumerate(ts_list):\n",
    "            ax = axes[ax_i]\n",
    "            ax.imshow(concat_frames, aspect=\"auto\")\n",
    "            ax.imshow(attn_up[ax_i].cpu(), aspect=\"auto\",\n",
    "                      cmap=cmap, alpha=alpha)\n",
    "            # ax.set_title(f\"at = {t}\", fontsize=10)\n",
    "            ax.axis(\"off\")\n",
    "        for ax in axes[n:]:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    else:  # mode == \"mean\"\n",
    "        attn_mean = attn_up.mean(0, keepdim=True)   # [1,H,W]\n",
    "        attn_mean = (attn_mean - attn_mean.min()) / (attn_mean.max() - attn_mean.min() + 1e-6)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(32, 4))\n",
    "        ax.imshow(concat_frames, aspect=\"auto\")\n",
    "        ax.imshow(attn_mean[0].cpu(), aspect=\"auto\",\n",
    "                  cmap=cmap, alpha=alpha)\n",
    "        # ax.set_title(\"Mean of all audio time-steps\", fontsize=12)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches=\"tight\", pad_inches=0)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "for layer_id in range(depth):\n",
    "    print(f\"Layer {layer_id+1}\")\n",
    "    \n",
    "    t2s = t2s_all_attn[layer_id]\n",
    "    t2s_split = rearrange(t2s,'b h (af at) (vn vt) -> b h af at vt vn',vt=T, vn=196, af=f_dim, at=t_dim)\n",
    "    t2s_split_attn_video = t2s_split.mean(-4).mean(1)\n",
    "    \n",
    "    save_dir = make_save_dir(batch, action, base_dir=\"./results\")\n",
    "    base_name = f\"layer{layer_id+1:02d}\"\n",
    "\n",
    "    # 1) 원본 비디오\n",
    "    spec_path = os.path.join(save_dir, f\"video.png\")\n",
    "    # save_rgb_image(concatenated_frames, spec_path)\n",
    "\n",
    "    overlay_path = os.path.join(save_dir, f\"{base_name}_video_overlay.png\")\n",
    "    show_t2s_attention_overlay(concatenated_frames,\n",
    "                            t2s_split_attn_video[0].detach(),\n",
    "                            mode=\"mean\",\n",
    "                            ts=[0,10,20,30],\n",
    "                            # save_path=overlay_path\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47b375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch, torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange            \n",
    "import gc\n",
    "\n",
    "def _prep_rgb(img):\n",
    "    \"\"\"RGB [H,W,3] → numpy.float32 in [0,1]\"\"\"\n",
    "    if torch.is_tensor(img):\n",
    "        img = img.detach().cpu().float().numpy()\n",
    "    if img.dtype == np.uint8:          # 이미 0–255 uint8\n",
    "        return img\n",
    "    # float32/64 → 0–1 로 스케일 or 클리핑\n",
    "    if img.min() < 0 or img.max() > 1:\n",
    "        img = (img - img.min()) / (img.max() - img.min() + 1e-6)\n",
    "    return np.clip(img, 0.0, 1.0)\n",
    "\n",
    "def _postprocess_heatmap(h, *, blur_ks=5, cutoff=0.2, gamma=2.0):\n",
    "    \"\"\"\n",
    "    h : [H,W] 0~1 텐서\n",
    "    blur_ks : 평균블러 커널(홀수). 0이면 블러 생략\n",
    "    cutoff  : (0~1) 하위 퍼센타일 절단. 0.2 → 하위 20% 0으로\n",
    "    gamma   : >1 이면 밝은 곳 강조, <1 이면 어두운 곳 강조\n",
    "    \"\"\"\n",
    "    if blur_ks and blur_ks > 1:\n",
    "        pad = blur_ks // 2\n",
    "        h = F.avg_pool2d(h.unsqueeze(0).unsqueeze(0), blur_ks,\n",
    "                         stride=1, padding=pad)[0,0]\n",
    "\n",
    "    if cutoff > 0:\n",
    "        thr = torch.quantile(h, cutoff)\n",
    "        h = torch.clamp(h - thr, min=0.0)\n",
    "\n",
    "    h = h / (h.max() + 1e-6)\n",
    "    if gamma != 1.0:\n",
    "        h = h.pow(gamma)\n",
    "    return h\n",
    "\n",
    "def _piecewise_strength(h, boundary=0.8,\n",
    "                        low_max=0.3, high_min=0.3, high_max=1.0):\n",
    "    \"\"\"\n",
    "    h : [H,W] 0–1 텐서\n",
    "    boundary : 경계값(예: 0.8 → 상위 20%)\n",
    "    low_max  : 경계 아래 구간의 최대 세기\n",
    "    high_min : 경계에서의 시작 세기\n",
    "    high_max : 1.0 에서의 최대 세기\n",
    "    \"\"\"\n",
    "    h_weighted = h.clone()\n",
    "    # ① 하위 구간: 0~boundary → 0~low_max 로 선형 스케일\n",
    "    low_mask = h < boundary\n",
    "    h_weighted[low_mask] = h_weighted[low_mask] / boundary * low_max\n",
    "\n",
    "    # ② 상위 구간: boundary~1 → high_min~high_max 선형 스케일\n",
    "    high_mask = ~low_mask\n",
    "    h_weighted[high_mask] = (\n",
    "        high_min +\n",
    "        (h_weighted[high_mask] - boundary) / (1 - boundary) * (high_max - high_min)\n",
    "    )\n",
    "    return h_weighted\n",
    "\n",
    "\n",
    "def show_t2s_attention_overlay(concat_frames: torch.Tensor | np.ndarray,\n",
    "                               attn_video: torch.Tensor,\n",
    "                               *,\n",
    "                               mode: str = \"per_ts\",      # \"per_ts\" | \"mean\"\n",
    "                               ts: list[int] | None = None,   # 보고 싶은 at 인덱스\n",
    "                               alpha: float = 0.4,\n",
    "                               cmap: str = \"jet\",\n",
    "                               blur_ks=5, cutoff=0.2, gamma=2.0,\n",
    "                               boundary=0.8, low_max=0.3, high_min=0.3, high_max=1.0,\n",
    "                               save_path=None):\n",
    "    \"\"\"\n",
    "    concat_frames : [224, 1792, 3]  (8 장의 224×224 프레임을 가로로 이어 붙인 RGB 이미지)\n",
    "    attn_video    : [39, 8, 196]    (at = 39, vt = 8, vn = 14×14)\n",
    "    \"\"\"\n",
    "    if mode not in {\"per_ts\", \"mean\"}:\n",
    "        raise ValueError(\"mode must be 'per_ts' or 'mean'\")\n",
    "\n",
    "    H, W = concat_frames.shape[:2]\n",
    "    device  = attn_video.device\n",
    "    concat_frames = _prep_rgb(concat_frames) \n",
    "    # (at, vt, vn=196) → (at, 14, 14*8=112)  패치 격자를 가로로 이어붙인 형태\n",
    "    attn_grid = rearrange(attn_video, 'at vt (ph pw) -> at ph (vt pw)', ph=14, pw=14)\n",
    "    # attn_grid = rearrange(attn_video, 'at vt (pw ph) -> at ph (vt pw)', ph=14, pw=14)\n",
    "    # ↑ [39,14,112]\n",
    "\n",
    "    # 업샘플 → [at, H, W]\n",
    "    attn_up = F.interpolate(attn_grid.unsqueeze(1), size=(H, W),\n",
    "                            mode=\"bilinear\", align_corners=False).squeeze(1)\n",
    "\n",
    "    if mode == \"per_ts\":\n",
    "        ts_list = ts if ts is not None else list(range(attn_up.size(0)))\n",
    "        # at 별로 0–1 정규화\n",
    "        min_val = torch.amin(attn_up[ts_list], dim=(-2, -1), keepdim=True)\n",
    "        max_val = torch.amax(attn_up[ts_list], dim=(-2, -1), keepdim=True)\n",
    "        attn_up = (attn_up[ts_list] - min_val) / (max_val - min_val + 1e-6)\n",
    "\n",
    "        n = len(ts_list)\n",
    "        cols = min(4, n); rows = math.ceil(n / cols)\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 3*rows),\n",
    "                                 sharex=True, sharey=True)\n",
    "        axes = axes.flatten() if n > 1 else [axes]\n",
    "\n",
    "        for ax_i, t in enumerate(ts_list):\n",
    "            ax = axes[ax_i]\n",
    "            ax.imshow(concat_frames, aspect=\"auto\")\n",
    "            ax.imshow(attn_up[ax_i].cpu(), aspect=\"auto\",\n",
    "                      cmap=cmap, alpha=alpha)\n",
    "            # ax.set_title(f\"at = {t}\", fontsize=10)\n",
    "            ax.axis(\"off\")\n",
    "        for ax in axes[n:]:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    else:  # mode == \"mean\"\n",
    "        h = attn_up.mean(0, keepdim=True)   # [1,H,W]\n",
    "        # h = ((h - h.min()) / (h.max() - h.min() + 1e-6))[0]\n",
    "        h = _postprocess_heatmap(h[0], blur_ks=blur_ks,\n",
    "                                 cutoff=cutoff, gamma=gamma)\n",
    "\n",
    "        h_strength = _piecewise_strength(h, boundary=boundary,\n",
    "                                 low_max=low_max,  # 0~0.8 구간\n",
    "                                 high_min=high_min, # 0.8 에서 시작\n",
    "                                 high_max=high_max) # 1.0 에서 최대\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(32, 4))\n",
    "        ax.imshow(concat_frames, aspect=\"auto\")\n",
    "        ax.imshow(h.cpu(), aspect=\"auto\",\n",
    "                  cmap=cmap, alpha=alpha)\n",
    "        # ax.set_title(\"Mean of all audio time-steps\", fontsize=12)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches=\"tight\", pad_inches=0)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    del attn_up, h\n",
    "\n",
    "\n",
    "for layer_id in range(depth):\n",
    "    print(f\"Layer {layer_id+1}\")\n",
    "    \n",
    "    t2s = t2s_all_attn[layer_id]\n",
    "    t2s_split = rearrange(t2s,'b h (af at) (vn vt) -> b h af at vt vn',vt=T, vn=196, af=f_dim, at=t_dim)\n",
    "    t2s_split_attn_video = t2s_split.mean(-4).mean(1)\n",
    "    \n",
    "    save_dir = make_save_dir(batch, action, base_dir=\"./results\")\n",
    "    base_name = f\"layer{layer_id+1:02d}\"\n",
    "\n",
    "    # 1) 원본 비디오\n",
    "    spec_path = os.path.join(save_dir, f\"video.png\")\n",
    "    # save_rgb_image(concatenated_frames, spec_path)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        overlay_path = os.path.join(save_dir, f\"{base_name}_video_overlay.png\")\n",
    "        show_t2s_attention_overlay(concatenated_frames,\n",
    "                                t2s_split_attn_video[0].detach(),\n",
    "                                mode=\"mean\",\n",
    "                            ts=[0,10,20,30],\n",
    "                            alpha=0.4,          # 전체 투명도\n",
    "                            blur_ks=1,          # 노이즈 스무딩\n",
    "                            cutoff=0,         # 하위 20% 제거\n",
    "                            gamma=0.5,          # 대비 ↑\n",
    "                            # save_path=overlay_path\n",
    "                            boundary=0.7, low_max=0.4, high_min=0.4, high_max=1.0\n",
    "                            )\n",
    "    \n",
    "    del t2s_split_attn_video\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e0a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch, torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_s2t_attention_waveform(waveform: torch.Tensor | np.ndarray,\n",
    "                                attn_audio: torch.Tensor,\n",
    "                                *,\n",
    "                                sr: int | None = None,          # 주파수(선택)\n",
    "                                mode: str = \"per_frame\",        # \"per_frame\" | \"mean\"\n",
    "                                frames: list[int] | None = None,# 보고 싶은 vt\n",
    "                                alpha: float = 0.4,\n",
    "                                cmap: str = \"jet\",\n",
    "                                save_path: str | None = None):\n",
    "    \"\"\"\n",
    "    waveform   : [S] 또는 [C,S]  (torch/np 모두OK)\n",
    "    attn_audio : [8, 12, 39]     (vt, f_dim, t_dim)\n",
    "\n",
    "    주 아이디어\n",
    "    1) attn_audio.mean(1)  -> [8,39]      # f_dim 축 평균\n",
    "    2) F.interpolate       -> [8,S]       # S = waveform length\n",
    "    3) imshow(extent=...)  로 α-블렌딩\n",
    "    \"\"\"\n",
    "    if mode not in {\"per_frame\", \"mean\"}:\n",
    "        raise ValueError(\"mode must be 'per_frame' or 'mean'\")\n",
    "\n",
    "    # ─── 데이터 준비 ─────────────────────────────────────────────\n",
    "    if torch.is_tensor(waveform):\n",
    "        wav = waveform.detach().cpu()\n",
    "    else:\n",
    "        wav = torch.as_tensor(waveform, dtype=torch.float32)\n",
    "    if wav.ndim == 2:       # stereo → 첫 채널\n",
    "        wav = wav[0]\n",
    "    S = wav.numel()\n",
    "    t = np.arange(S) / sr if sr else np.arange(S)   # x축\n",
    "\n",
    "    # attn: [vt, 39] → upsample → [vt, S]\n",
    "    attn = attn_audio.mean(1)                       # [8,39]\n",
    "    attn_up = F.interpolate(attn.unsqueeze(1), size=S,\n",
    "                            mode='linear', align_corners=False).squeeze(1)  # [8,S]\n",
    "\n",
    "    # ─── overlay ────────────────────────────────────────────────\n",
    "    if mode == \"per_frame\":\n",
    "        vt_list = frames if frames is not None else list(range(attn_up.size(0)))\n",
    "        n, cols = len(vt_list), min(4, len(vt_list))\n",
    "        rows = math.ceil(n / cols)\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 2.5*rows),\n",
    "                                 sharex=True, sharey=True)\n",
    "        axes = axes.flatten() if n > 1 else [axes]\n",
    "\n",
    "        for ax_i, vt in enumerate(vt_list):\n",
    "            ax = axes[ax_i]\n",
    "            # wave\n",
    "            ax.plot(t, wav, lw=0.7, color='black')\n",
    "            # attn heatmap (0–1 정규화)\n",
    "            h = (attn_up[vt] - attn_up[vt].min()) / (attn_up[vt].max() - attn_up[vt].min() + 1e-6)\n",
    "            ax.imshow(h.unsqueeze(0).repeat(50,1),   # 높이 50px dummy\n",
    "                      extent=[t[0], t[-1], wav.min(), wav.max()],\n",
    "                      origin='lower', aspect='auto',\n",
    "                      cmap=cmap, alpha=alpha)\n",
    "            ax.set_title(f\"vt={vt}\", fontsize=9)\n",
    "            ax.axis(\"off\")\n",
    "        for ax in axes[n:]:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    else:  # mode == \"mean\"\n",
    "        h = attn_up.mean(0)\n",
    "        h = (h - h.min()) / (h.max() - h.min() + 1e-6)\n",
    "        fig, ax = plt.subplots(figsize=(8, 2.5))\n",
    "        ax.plot(t, wav, lw=0.7, color='black')\n",
    "        ax.imshow(h.unsqueeze(0).repeat(50,1),\n",
    "                  extent=[t[0], t[-1], wav.min(), wav.max()],\n",
    "                  origin='lower', aspect='auto',\n",
    "                  cmap=cmap, alpha=alpha)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "for layer_id in range(depth):\n",
    "    print(f\"Layer {layer_id+1}\")\n",
    "    s2t = s2t_all_attn[layer_id]\n",
    "    s2t_split = rearrange(s2t,'b h (vn vt) (af at) -> b h vt vn af at',vt=T, vn=196, af=f_dim, at=t_dim)\n",
    "    s2t_split_attn_audio = s2t_split.mean(-3).mean(1)\n",
    "    \n",
    "    save_dir = make_save_dir(batch, action, base_dir=\"./results\")\n",
    "    base_name = f\"layer{layer_id+1:02d}\"\n",
    "\n",
    "    # 1) 원본 스펙트로그램\n",
    "    # spec_path = os.path.join(save_dir, f\"spec.png\")\n",
    "    # save_audio_spec(audio_spec.detach(), spec_path)\n",
    "\n",
    "    # 2) overlay (mean 모드 예시)\n",
    "    # overlay_path = os.path.join(save_dir, f\"{base_name}_spec_overlay.png\")\n",
    "    show_s2t_attention_waveform(audio_wav,\n",
    "                               s2t_split_attn_audio[0].detach().cpu(),\n",
    "                               mode='mean',\n",
    "                            #    save_path=overlay_path\n",
    "                               )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230cb483",
   "metadata": {},
   "source": [
    "# Multi Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5a3ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbb0a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from models.prompt import dataset_class\n",
    "action = dataset_class(time_args.data_set, time_args.anno_path)\n",
    "\n",
    "time_model.eval()\n",
    "device = \"cuda\"\n",
    "\n",
    "N = 200  # 평가할 샘플 수 (원하면 더 늘리기)\n",
    "layer_depth = len(time_model.blocks)  # = len(all_atts)//2  와 동일\n",
    "\n",
    "# [layer][sample] 구조로 entropies 저장\n",
    "s2t_all = [[] for _ in range(layer_depth)]\n",
    "t2s_all = [[] for _ in range(layer_depth)]\n",
    "s2t_all_attn = []\n",
    "t2s_all_attn = []\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def compute_entropy(attn):\n",
    "#     p = attn.clamp_min(1e-12)\n",
    "#     h = -(p * p.log2()).sum(-1)       # [B, head, query]\n",
    "#     return h.mean().item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_entropy(attn, mode=\"ratio\"):\n",
    "    p = attn.clamp_min(1e-12)\n",
    "    h = -(p * p.log2()).sum(-1)          # [B, head, Q]\n",
    "\n",
    "    if mode == \"norm\":       # 0~1\n",
    "        K = attn.size(-1)\n",
    "        h = h / math.log2(K)\n",
    "    elif mode == \"ratio\":    # 0~1, effective key share\n",
    "        K = attn.size(-1)\n",
    "        h = (2**h) / K\n",
    "\n",
    "    return h.mean().item()\n",
    "\n",
    "# 전체 데이터셋에서 N개 랜덤 인덱스 선택\n",
    "total_samples = len(data_loader_val)\n",
    "rand_indices = np.random.choice(total_samples, N, replace=False)\n",
    "ind_correct = []\n",
    "\n",
    "for idx in rand_indices:\n",
    "    batch = data_loader_val.get_item_by_index(idx)\n",
    "\n",
    "    time_idx = torch.tensor(batch[5]).unsqueeze(0).to(device)\n",
    "    samples  = batch[0].unsqueeze(0).to(device, non_blocking=True)\n",
    "    spec     = batch[3].unsqueeze(0).to(device, non_blocking=True)\n",
    "    captions = batch[4]\n",
    "\n",
    "    logits, all_atts = time_model(samples,\n",
    "                             caption=captions,\n",
    "                             spec=spec,\n",
    "                             idx=time_idx,\n",
    "                             output_attentions=True)\n",
    "\n",
    "    print('action :', action['action'][batch[1]], '\\tpred :', action['action'][torch.argmax(logits).item()])\n",
    "    ind_correct.append(action['action'][batch[1]] == action['action'][torch.argmax(logits).item()])\n",
    "\n",
    "    for l in range(layer_depth):\n",
    "        t2s = all_atts[2*l]      # video Q ← audio KV\n",
    "        s2t = all_atts[2*l + 1]  # audio Q ← video KV\n",
    "\n",
    "        s2t_all[l].append(compute_entropy(s2t.detach().cpu()))\n",
    "        t2s_all[l].append(compute_entropy(t2s.detach().cpu()))\n",
    "        # s2t_all_attn.append(s2t.detach().cpu())\n",
    "        # t2s_all_attn.append(t2s.detach().cpu())\n",
    "\n",
    "# numpy 로 평균·표준편차 계산\n",
    "mean_s2t = [np.mean(x) for x in s2t_all]\n",
    "std_s2t  = [np.std(x)  for x in s2t_all]\n",
    "mean_t2s = [np.mean(x) for x in t2s_all]\n",
    "std_t2s  = [np.std(x)  for x in t2s_all]\n",
    "\n",
    "print(f\"Correct predictions: {sum(ind_correct)}/{N} ({100 * sum(ind_correct) / N:.2f}%)\")\n",
    "\n",
    "print(f\"\\nEvaluated {N} samples\")\n",
    "print(\"Layer |  S2T  μ±σ   |  T2S  μ±σ\")\n",
    "print(\"----------------------------------\")\n",
    "for i, (m1, s1, m2, s2) in enumerate(zip(mean_s2t, std_s2t, mean_t2s, std_t2s), 1):\n",
    "    print(f\"{i:2d}   | {m1:5.2f}±{s1:4.2f} | {m2:5.2f}±{s2:4.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b3d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "layers = list(range(1, len(mean_s2t)+1))\n",
    "df = pd.DataFrame({\n",
    "    \"layer\": layers,\n",
    "    \"mean_A2V_ratio\": mean_s2t,\n",
    "    \"std_A2V_ratio\": std_s2t,\n",
    "    \"mean_V2A_ratio\": mean_t2s,\n",
    "    \"std_V2A_ratio\": std_t2s\n",
    "})\n",
    "\n",
    "file_path = \"cava_entropy_stats_vggsoud_speccut_missing0.csv\"\n",
    "# df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"CSV saved at {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed0e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# Helvetica.ttf 폰트 경로 지정 및 FontEntry 등록\n",
    "font_path = '/data/joohyun7u/.fonts/Helvetica.ttf'\n",
    "fm.fontManager.addfont(font_path)   # <- 추가\n",
    "\n",
    "# 폰트 이름을 직접 지정 (ttf 내의 family name이어야 함)\n",
    "prop = fm.FontProperties(fname=font_path)\n",
    "# plt.rcParams['font.family'] = prop.get_name()\n",
    "# plt.rcParams['font.family'] = [prop.get_name(), 'DejaVu Sans', 'Noto Sans Symbols']\n",
    "symbol_font = fm.FontProperties(family='DejaVu Sans')  # 시스템\n",
    "\n",
    "# rcParams에 지정한 이름 등록\n",
    "plt.rcParams['font.family'] = 'Helvetica'\n",
    "\n",
    "sel_type = 1\n",
    "if sel_type == 1:\n",
    "    mean_t2s = [np.mean(x) for x in t2s_all]\n",
    "    std_t2s  = [np.std(x)  for x in t2s_all]\n",
    "    mean_s2t = [np.mean(x) for x in s2t_all]\n",
    "    std_s2t  = [np.std(x)  for x in s2t_all]\n",
    "elif sel_type >= 2:\n",
    "    import pandas as pd\n",
    "    # entropy_path = './cava_entropy_stats_missing0.csv'\n",
    "    entropy_path = './cava_entropy_stats_epicsound_pth3_missing0.csv'\n",
    "    # entropy_path = './cava_entropy_stats_vggsound_missing0.csv'\n",
    "    entropy_df = pd.read_csv(entropy_path)\n",
    "    mean_t2s = entropy_df['mean_V2A_ratio'].tolist()\n",
    "    mean_s2t = entropy_df['mean_A2V_ratio'].tolist()\n",
    "    std_t2s  = entropy_df['std_V2A_ratio'].tolist()\n",
    "    std_s2t  = entropy_df['std_A2V_ratio'].tolist()\n",
    "\n",
    "layers = np.arange(1, len(mean_t2s) + 1)\n",
    "# layers = np.arange(0, len(mean_t2s) + 0)\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.errorbar(layers, mean_t2s, yerr=std_t2s, fmt='o-', capsize=4, label='V' + r\"$\\rightarrow$\" + 'A Entropy')\n",
    "plt.errorbar(layers, mean_s2t, yerr=std_s2t, fmt='s-', capsize=4, label='A' + r\"$\\rightarrow$\" + 'V Entropy')\n",
    "plt.xlabel('Layer', fontsize=14)\n",
    "plt.ylabel('Entropy Ratio', fontsize=14)\n",
    "plt.title('Layer-wise Attention Entropy (V' + r\"$\\rightarrow$\" + 'A & A' + r\"$\\rightarrow$\" + 'V)', fontsize=16)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "# plt.savefig(\"layerwise_entropy_vggsoud_speccut_missing0.pdf\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# # entropy ratio로 나눈 값 시각화\n",
    "# mean_t2s_ratio = [m / np.log2(time_model.blocks[0].cross_t_down.out_features) for m in mean_t2s]\n",
    "# mean_s2t_ratio = [m / np.log2(time_model.blocks[0].cross_s_down.out_features) for m in mean_s2t]\n",
    "\n",
    "# plt.figure(figsize=(12,5))\n",
    "# plt.errorbar(layers, mean_t2s_ratio, yerr=std_t2s, fmt='o-', capsize=4, label='T→S Entropy Ratio')\n",
    "# plt.errorbar(layers, mean_s2t_ratio, yerr=std_s2t, fmt='s-', capsize=4, label='S→T Entropy Ratio')\n",
    "# plt.xlabel('Layer')\n",
    "# plt.ylabel('Entropy Ratio')\n",
    "# plt.title('Layer-wise Attention Entropy Ratio (T→S & S→T)')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "# # plt.savefig(\"layerwise_entropy_ratio.pdf\", bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c660b151",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ca2st",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
